{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzQfE7FHRts6Fhcc0kx50l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphaelp-silva/deep_learning_com_pytorch_e_python/blob/main/Salvando_e_carregando_o_modelo_(ex7)_e_classificando_somente_um_registro_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importando as bibliotecas"
      ],
      "metadata": {
        "id": "4Ab726NPNhev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e3hARB_MKdds",
        "outputId": "877f1b44-87d1-430d-9831-1e5c9167f33b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base de dados"
      ],
      "metadata": {
        "id": "GUC8FTDmNlls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CotFb6peKsO9",
        "outputId": "5db9b57f-b7b4-4c2c-e2ea-3273910d93e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78465f42f770>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base = pd.read_csv('/content/iris.csv')\n",
        "previsores = base.iloc[:, 0:4].values #selecionando todas as linhas e as 3 primeiras colunas\n",
        "classe = base.iloc[:, 4].values #selecionando todas as linhas e apenas a 4 coluna"
      ],
      "metadata": {
        "id": "dx2kVFduK3-T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertendo valores categóricos em numéricos (ex: flor -> 1, petala -> 2)\n",
        "encoder = LabelEncoder()\n",
        "classe = encoder.fit_transform(classe)"
      ],
      "metadata": {
        "id": "zi5KWVonMoWB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformando de np array para tensores:\n",
        "previsores = torch.tensor(np.array(previsores), dtype = torch.float)\n",
        "classe = torch.tensor(np.array(classe), dtype = torch.float)"
      ],
      "metadata": {
        "id": "chnC65bYLvou"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previsores.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrWDYMUgMMUH",
        "outputId": "58ab2d68-ec66-45f0-9a3c-79aa2b2bed88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([150, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classe.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBtcLxB1NWnT",
        "outputId": "08fb1aeb-43a0-4398-a403-8d68c29de7d9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([150])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando um dataset com previsores e classe"
      ],
      "metadata": {
        "id": "eXMuKRwRNXtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(previsores, classe), # criando um dataset com o TensorDataset agrupando as entradas(previsores) e os rótulos (classe)\n",
        "                                           batch_size = 10, # cada batch conterá 10 amostras\n",
        "                                           shuffle = True) # os dados serão embaralhados a cada épooca, garantindo que o modelo não decore a ordem dos dados\n",
        ""
      ],
      "metadata": {
        "id": "b6UxpkvZNabD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construção do modelo"
      ],
      "metadata": {
        "id": "h3MeMxvXOnZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificadorTorch (nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense0 = nn.Linear(4,16)\n",
        "    torch.nn.init.uniform_(self.dense0.weight, -0.05, 0.05)\n",
        "    self.dense1 = nn.Linear(16,16)\n",
        "    torch.nn.init.uniform_(self.dense1.weight, -0.05, 0.05)\n",
        "    self.dense2 = nn.Linear(16,16)\n",
        "    torch.nn.init.uniform_(self.dense2.weight, -0.05, 0.05)\n",
        "    self.dense3 = nn.Linear(16,3)\n",
        "\n",
        "    self.activation = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.output = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = self.dense0(X)\n",
        "    X = self.activation(X)\n",
        "    X = self.dropout(X)\n",
        "    X = self.dense1(X)\n",
        "    X = self.activation(X)\n",
        "    X = self.dropout(X)\n",
        "    X = self.dense2(X)\n",
        "    X = self.activation(X)\n",
        "    X = self.dropout(X)\n",
        "    X = self.dense3(X)\n",
        "    X = self.output(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "onkEXsjVOpJ7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificador = ClassificadorTorch()"
      ],
      "metadata": {
        "id": "NVnG7oL9R5CM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classificador.parameters(),\n",
        "                             lr = 0.001,\n",
        "                             weight_decay = 0.0001)"
      ],
      "metadata": {
        "id": "TZF6fouBR8gQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o modelo"
      ],
      "metadata": {
        "id": "W2rJHYXVSQHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range (500):\n",
        "  running_loss = 0.\n",
        "\n",
        "  for data in train_loader:\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = classificador(inputs)\n",
        "    loss = criterion(outputs, labels.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print(f'Época {epoch+1:3d}: perda {running_loss/len(train_loader):5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEIKGgOSSwh",
        "outputId": "437123be-88f9-4f77-824f-c2044c04bef4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época   1: perda 1.099640\n",
            "Época   2: perda 1.098976\n",
            "Época   3: perda 1.098895\n",
            "Época   4: perda 1.098685\n",
            "Época   5: perda 1.098947\n",
            "Época   6: perda 1.098454\n",
            "Época   7: perda 1.097411\n",
            "Época   8: perda 1.094959\n",
            "Época   9: perda 1.087593\n",
            "Época  10: perda 1.071121\n",
            "Época  11: perda 1.046149\n",
            "Época  12: perda 1.020933\n",
            "Época  13: perda 1.003415\n",
            "Época  14: perda 0.969156\n",
            "Época  15: perda 0.929676\n",
            "Época  16: perda 0.903507\n",
            "Época  17: perda 0.873580\n",
            "Época  18: perda 0.850458\n",
            "Época  19: perda 0.847349\n",
            "Época  20: perda 0.836337\n",
            "Época  21: perda 0.812754\n",
            "Época  22: perda 0.810298\n",
            "Época  23: perda 0.810559\n",
            "Época  24: perda 0.802541\n",
            "Época  25: perda 0.805113\n",
            "Época  26: perda 0.769174\n",
            "Época  27: perda 0.767507\n",
            "Época  28: perda 0.750703\n",
            "Época  29: perda 0.756477\n",
            "Época  30: perda 0.739609\n",
            "Época  31: perda 0.745320\n",
            "Época  32: perda 0.710760\n",
            "Época  33: perda 0.731333\n",
            "Época  34: perda 0.723003\n",
            "Época  35: perda 0.705042\n",
            "Época  36: perda 0.688156\n",
            "Época  37: perda 0.693372\n",
            "Época  38: perda 0.704842\n",
            "Época  39: perda 0.681564\n",
            "Época  40: perda 0.682657\n",
            "Época  41: perda 0.661644\n",
            "Época  42: perda 0.689054\n",
            "Época  43: perda 0.666961\n",
            "Época  44: perda 0.640585\n",
            "Época  45: perda 0.667324\n",
            "Época  46: perda 0.662372\n",
            "Época  47: perda 0.631393\n",
            "Época  48: perda 0.636769\n",
            "Época  49: perda 0.630034\n",
            "Época  50: perda 0.645454\n",
            "Época  51: perda 0.646843\n",
            "Época  52: perda 0.649828\n",
            "Época  53: perda 0.668058\n",
            "Época  54: perda 0.630752\n",
            "Época  55: perda 0.635024\n",
            "Época  56: perda 0.639355\n",
            "Época  57: perda 0.637666\n",
            "Época  58: perda 0.637423\n",
            "Época  59: perda 0.640823\n",
            "Época  60: perda 0.617350\n",
            "Época  61: perda 0.612764\n",
            "Época  62: perda 0.638210\n",
            "Época  63: perda 0.607792\n",
            "Época  64: perda 0.616897\n",
            "Época  65: perda 0.624150\n",
            "Época  66: perda 0.619738\n",
            "Época  67: perda 0.625129\n",
            "Época  68: perda 0.605179\n",
            "Época  69: perda 0.629049\n",
            "Época  70: perda 0.616133\n",
            "Época  71: perda 0.637949\n",
            "Época  72: perda 0.629214\n",
            "Época  73: perda 0.618189\n",
            "Época  74: perda 0.624534\n",
            "Época  75: perda 0.611011\n",
            "Época  76: perda 0.620437\n",
            "Época  77: perda 0.622824\n",
            "Época  78: perda 0.629365\n",
            "Época  79: perda 0.632773\n",
            "Época  80: perda 0.625205\n",
            "Época  81: perda 0.633108\n",
            "Época  82: perda 0.595068\n",
            "Época  83: perda 0.609603\n",
            "Época  84: perda 0.618425\n",
            "Época  85: perda 0.621895\n",
            "Época  86: perda 0.614050\n",
            "Época  87: perda 0.623903\n",
            "Época  88: perda 0.600962\n",
            "Época  89: perda 0.615888\n",
            "Época  90: perda 0.612436\n",
            "Época  91: perda 0.616993\n",
            "Época  92: perda 0.619066\n",
            "Época  93: perda 0.595649\n",
            "Época  94: perda 0.617052\n",
            "Época  95: perda 0.608607\n",
            "Época  96: perda 0.602737\n",
            "Época  97: perda 0.619830\n",
            "Época  98: perda 0.606728\n",
            "Época  99: perda 0.617193\n",
            "Época 100: perda 0.613838\n",
            "Época 101: perda 0.601544\n",
            "Época 102: perda 0.608066\n",
            "Época 103: perda 0.605658\n",
            "Época 104: perda 0.621866\n",
            "Época 105: perda 0.600857\n",
            "Época 106: perda 0.602608\n",
            "Época 107: perda 0.597172\n",
            "Época 108: perda 0.592523\n",
            "Época 109: perda 0.620143\n",
            "Época 110: perda 0.600709\n",
            "Época 111: perda 0.594268\n",
            "Época 112: perda 0.614722\n",
            "Época 113: perda 0.601121\n",
            "Época 114: perda 0.584456\n",
            "Época 115: perda 0.609398\n",
            "Época 116: perda 0.597307\n",
            "Época 117: perda 0.602072\n",
            "Época 118: perda 0.606044\n",
            "Época 119: perda 0.625088\n",
            "Época 120: perda 0.626458\n",
            "Época 121: perda 0.608457\n",
            "Época 122: perda 0.621079\n",
            "Época 123: perda 0.598320\n",
            "Época 124: perda 0.579636\n",
            "Época 125: perda 0.595443\n",
            "Época 126: perda 0.613446\n",
            "Época 127: perda 0.596032\n",
            "Época 128: perda 0.602993\n",
            "Época 129: perda 0.595634\n",
            "Época 130: perda 0.593258\n",
            "Época 131: perda 0.611343\n",
            "Época 132: perda 0.586552\n",
            "Época 133: perda 0.588542\n",
            "Época 134: perda 0.587134\n",
            "Época 135: perda 0.594920\n",
            "Época 136: perda 0.606509\n",
            "Época 137: perda 0.595659\n",
            "Época 138: perda 0.597144\n",
            "Época 139: perda 0.580883\n",
            "Época 140: perda 0.607771\n",
            "Época 141: perda 0.599518\n",
            "Época 142: perda 0.589881\n",
            "Época 143: perda 0.599219\n",
            "Época 144: perda 0.593643\n",
            "Época 145: perda 0.588335\n",
            "Época 146: perda 0.599383\n",
            "Época 147: perda 0.587818\n",
            "Época 148: perda 0.597708\n",
            "Época 149: perda 0.589146\n",
            "Época 150: perda 0.582468\n",
            "Época 151: perda 0.587602\n",
            "Época 152: perda 0.586439\n",
            "Época 153: perda 0.597414\n",
            "Época 154: perda 0.583162\n",
            "Época 155: perda 0.595632\n",
            "Época 156: perda 0.587540\n",
            "Época 157: perda 0.584945\n",
            "Época 158: perda 0.599045\n",
            "Época 159: perda 0.580620\n",
            "Época 160: perda 0.599978\n",
            "Época 161: perda 0.598566\n",
            "Época 162: perda 0.600013\n",
            "Época 163: perda 0.606193\n",
            "Época 164: perda 0.596428\n",
            "Época 165: perda 0.593238\n",
            "Época 166: perda 0.586521\n",
            "Época 167: perda 0.590256\n",
            "Época 168: perda 0.577814\n",
            "Época 169: perda 0.588509\n",
            "Época 170: perda 0.606638\n",
            "Época 171: perda 0.595079\n",
            "Época 172: perda 0.584113\n",
            "Época 173: perda 0.593019\n",
            "Época 174: perda 0.602295\n",
            "Época 175: perda 0.577804\n",
            "Época 176: perda 0.599759\n",
            "Época 177: perda 0.612708\n",
            "Época 178: perda 0.580692\n",
            "Época 179: perda 0.594049\n",
            "Época 180: perda 0.591996\n",
            "Época 181: perda 0.606510\n",
            "Época 182: perda 0.593304\n",
            "Época 183: perda 0.590605\n",
            "Época 184: perda 0.574830\n",
            "Época 185: perda 0.587068\n",
            "Época 186: perda 0.578638\n",
            "Época 187: perda 0.579076\n",
            "Época 188: perda 0.594830\n",
            "Época 189: perda 0.587460\n",
            "Época 190: perda 0.588595\n",
            "Época 191: perda 0.601427\n",
            "Época 192: perda 0.605957\n",
            "Época 193: perda 0.589233\n",
            "Época 194: perda 0.589503\n",
            "Época 195: perda 0.588447\n",
            "Época 196: perda 0.584682\n",
            "Época 197: perda 0.572013\n",
            "Época 198: perda 0.591910\n",
            "Época 199: perda 0.581496\n",
            "Época 200: perda 0.582546\n",
            "Época 201: perda 0.586381\n",
            "Época 202: perda 0.575145\n",
            "Época 203: perda 0.595925\n",
            "Época 204: perda 0.590135\n",
            "Época 205: perda 0.582936\n",
            "Época 206: perda 0.595933\n",
            "Época 207: perda 0.580260\n",
            "Época 208: perda 0.577505\n",
            "Época 209: perda 0.588437\n",
            "Época 210: perda 0.601267\n",
            "Época 211: perda 0.580373\n",
            "Época 212: perda 0.593274\n",
            "Época 213: perda 0.588601\n",
            "Época 214: perda 0.582231\n",
            "Época 215: perda 0.593851\n",
            "Época 216: perda 0.594080\n",
            "Época 217: perda 0.589345\n",
            "Época 218: perda 0.598916\n",
            "Época 219: perda 0.595967\n",
            "Época 220: perda 0.580957\n",
            "Época 221: perda 0.579194\n",
            "Época 222: perda 0.580364\n",
            "Época 223: perda 0.598913\n",
            "Época 224: perda 0.603971\n",
            "Época 225: perda 0.589321\n",
            "Época 226: perda 0.588478\n",
            "Época 227: perda 0.597498\n",
            "Época 228: perda 0.589542\n",
            "Época 229: perda 0.583814\n",
            "Época 230: perda 0.601929\n",
            "Época 231: perda 0.594893\n",
            "Época 232: perda 0.592126\n",
            "Época 233: perda 0.584100\n",
            "Época 234: perda 0.598304\n",
            "Época 235: perda 0.585379\n",
            "Época 236: perda 0.591464\n",
            "Época 237: perda 0.586169\n",
            "Época 238: perda 0.578524\n",
            "Época 239: perda 0.595694\n",
            "Época 240: perda 0.594769\n",
            "Época 241: perda 0.594998\n",
            "Época 242: perda 0.591820\n",
            "Época 243: perda 0.587080\n",
            "Época 244: perda 0.597294\n",
            "Época 245: perda 0.577961\n",
            "Época 246: perda 0.574412\n",
            "Época 247: perda 0.568830\n",
            "Época 248: perda 0.600271\n",
            "Época 249: perda 0.583367\n",
            "Época 250: perda 0.577927\n",
            "Época 251: perda 0.576493\n",
            "Época 252: perda 0.587650\n",
            "Época 253: perda 0.609943\n",
            "Época 254: perda 0.585578\n",
            "Época 255: perda 0.569745\n",
            "Época 256: perda 0.584134\n",
            "Época 257: perda 0.579981\n",
            "Época 258: perda 0.576198\n",
            "Época 259: perda 0.590300\n",
            "Época 260: perda 0.597805\n",
            "Época 261: perda 0.580524\n",
            "Época 262: perda 0.581945\n",
            "Época 263: perda 0.573924\n",
            "Época 264: perda 0.573915\n",
            "Época 265: perda 0.572023\n",
            "Época 266: perda 0.577546\n",
            "Época 267: perda 0.585836\n",
            "Época 268: perda 0.585602\n",
            "Época 269: perda 0.581187\n",
            "Época 270: perda 0.589479\n",
            "Época 271: perda 0.594907\n",
            "Época 272: perda 0.596430\n",
            "Época 273: perda 0.580720\n",
            "Época 274: perda 0.581913\n",
            "Época 275: perda 0.583994\n",
            "Época 276: perda 0.586115\n",
            "Época 277: perda 0.576304\n",
            "Época 278: perda 0.585663\n",
            "Época 279: perda 0.585676\n",
            "Época 280: perda 0.596968\n",
            "Época 281: perda 0.576129\n",
            "Época 282: perda 0.590964\n",
            "Época 283: perda 0.579506\n",
            "Época 284: perda 0.602363\n",
            "Época 285: perda 0.602576\n",
            "Época 286: perda 0.586961\n",
            "Época 287: perda 0.584463\n",
            "Época 288: perda 0.591195\n",
            "Época 289: perda 0.582694\n",
            "Época 290: perda 0.589984\n",
            "Época 291: perda 0.568772\n",
            "Época 292: perda 0.587069\n",
            "Época 293: perda 0.583027\n",
            "Época 294: perda 0.598831\n",
            "Época 295: perda 0.586268\n",
            "Época 296: perda 0.598456\n",
            "Época 297: perda 0.579228\n",
            "Época 298: perda 0.593154\n",
            "Época 299: perda 0.589412\n",
            "Época 300: perda 0.581799\n",
            "Época 301: perda 0.577314\n",
            "Época 302: perda 0.582680\n",
            "Época 303: perda 0.593171\n",
            "Época 304: perda 0.588032\n",
            "Época 305: perda 0.584463\n",
            "Época 306: perda 0.574961\n",
            "Época 307: perda 0.586435\n",
            "Época 308: perda 0.580633\n",
            "Época 309: perda 0.581783\n",
            "Época 310: perda 0.575872\n",
            "Época 311: perda 0.574168\n",
            "Época 312: perda 0.584285\n",
            "Época 313: perda 0.585638\n",
            "Época 314: perda 0.580889\n",
            "Época 315: perda 0.585453\n",
            "Época 316: perda 0.580252\n",
            "Época 317: perda 0.585195\n",
            "Época 318: perda 0.598827\n",
            "Época 319: perda 0.575405\n",
            "Época 320: perda 0.599035\n",
            "Época 321: perda 0.580727\n",
            "Época 322: perda 0.572992\n",
            "Época 323: perda 0.586618\n",
            "Época 324: perda 0.590508\n",
            "Época 325: perda 0.591221\n",
            "Época 326: perda 0.596911\n",
            "Época 327: perda 0.593974\n",
            "Época 328: perda 0.579332\n",
            "Época 329: perda 0.567921\n",
            "Época 330: perda 0.590536\n",
            "Época 331: perda 0.595526\n",
            "Época 332: perda 0.590906\n",
            "Época 333: perda 0.589151\n",
            "Época 334: perda 0.575386\n",
            "Época 335: perda 0.585783\n",
            "Época 336: perda 0.587151\n",
            "Época 337: perda 0.581872\n",
            "Época 338: perda 0.583792\n",
            "Época 339: perda 0.593322\n",
            "Época 340: perda 0.580649\n",
            "Época 341: perda 0.571323\n",
            "Época 342: perda 0.589455\n",
            "Época 343: perda 0.600251\n",
            "Época 344: perda 0.588740\n",
            "Época 345: perda 0.583467\n",
            "Época 346: perda 0.601057\n",
            "Época 347: perda 0.577560\n",
            "Época 348: perda 0.594683\n",
            "Época 349: perda 0.581498\n",
            "Época 350: perda 0.577218\n",
            "Época 351: perda 0.579100\n",
            "Época 352: perda 0.586891\n",
            "Época 353: perda 0.587641\n",
            "Época 354: perda 0.584166\n",
            "Época 355: perda 0.581040\n",
            "Época 356: perda 0.581876\n",
            "Época 357: perda 0.572107\n",
            "Época 358: perda 0.590564\n",
            "Época 359: perda 0.587414\n",
            "Época 360: perda 0.575275\n",
            "Época 361: perda 0.579008\n",
            "Época 362: perda 0.587670\n",
            "Época 363: perda 0.577647\n",
            "Época 364: perda 0.579893\n",
            "Época 365: perda 0.577551\n",
            "Época 366: perda 0.589827\n",
            "Época 367: perda 0.593386\n",
            "Época 368: perda 0.590084\n",
            "Época 369: perda 0.587339\n",
            "Época 370: perda 0.590775\n",
            "Época 371: perda 0.586602\n",
            "Época 372: perda 0.581578\n",
            "Época 373: perda 0.584555\n",
            "Época 374: perda 0.580198\n",
            "Época 375: perda 0.587242\n",
            "Época 376: perda 0.575510\n",
            "Época 377: perda 0.587772\n",
            "Época 378: perda 0.589910\n",
            "Época 379: perda 0.574671\n",
            "Época 380: perda 0.588214\n",
            "Época 381: perda 0.595023\n",
            "Época 382: perda 0.600233\n",
            "Época 383: perda 0.597754\n",
            "Época 384: perda 0.595353\n",
            "Época 385: perda 0.578398\n",
            "Época 386: perda 0.594473\n",
            "Época 387: perda 0.593185\n",
            "Época 388: perda 0.588379\n",
            "Época 389: perda 0.593168\n",
            "Época 390: perda 0.600694\n",
            "Época 391: perda 0.574798\n",
            "Época 392: perda 0.592302\n",
            "Época 393: perda 0.586748\n",
            "Época 394: perda 0.576125\n",
            "Época 395: perda 0.590082\n",
            "Época 396: perda 0.595476\n",
            "Época 397: perda 0.606475\n",
            "Época 398: perda 0.593255\n",
            "Época 399: perda 0.570660\n",
            "Época 400: perda 0.590097\n",
            "Época 401: perda 0.600535\n",
            "Época 402: perda 0.581974\n",
            "Época 403: perda 0.599354\n",
            "Época 404: perda 0.580258\n",
            "Época 405: perda 0.577005\n",
            "Época 406: perda 0.581036\n",
            "Época 407: perda 0.586143\n",
            "Época 408: perda 0.587938\n",
            "Época 409: perda 0.577940\n",
            "Época 410: perda 0.575093\n",
            "Época 411: perda 0.578329\n",
            "Época 412: perda 0.572124\n",
            "Época 413: perda 0.578782\n",
            "Época 414: perda 0.575718\n",
            "Época 415: perda 0.570972\n",
            "Época 416: perda 0.583877\n",
            "Época 417: perda 0.586160\n",
            "Época 418: perda 0.585121\n",
            "Época 419: perda 0.605934\n",
            "Época 420: perda 0.583236\n",
            "Época 421: perda 0.588688\n",
            "Época 422: perda 0.592167\n",
            "Época 423: perda 0.591889\n",
            "Época 424: perda 0.587438\n",
            "Época 425: perda 0.583874\n",
            "Época 426: perda 0.591560\n",
            "Época 427: perda 0.576004\n",
            "Época 428: perda 0.584443\n",
            "Época 429: perda 0.582004\n",
            "Época 430: perda 0.576768\n",
            "Época 431: perda 0.599581\n",
            "Época 432: perda 0.608023\n",
            "Época 433: perda 0.581533\n",
            "Época 434: perda 0.583412\n",
            "Época 435: perda 0.597419\n",
            "Época 436: perda 0.580837\n",
            "Época 437: perda 0.577629\n",
            "Época 438: perda 0.560235\n",
            "Época 439: perda 0.584475\n",
            "Época 440: perda 0.571858\n",
            "Época 441: perda 0.583209\n",
            "Época 442: perda 0.585329\n",
            "Época 443: perda 0.599177\n",
            "Época 444: perda 0.591731\n",
            "Época 445: perda 0.579565\n",
            "Época 446: perda 0.571596\n",
            "Época 447: perda 0.601234\n",
            "Época 448: perda 0.580226\n",
            "Época 449: perda 0.585024\n",
            "Época 450: perda 0.583909\n",
            "Época 451: perda 0.574960\n",
            "Época 452: perda 0.583147\n",
            "Época 453: perda 0.578850\n",
            "Época 454: perda 0.578609\n",
            "Época 455: perda 0.583671\n",
            "Época 456: perda 0.604467\n",
            "Época 457: perda 0.579815\n",
            "Época 458: perda 0.592160\n",
            "Época 459: perda 0.578782\n",
            "Época 460: perda 0.578562\n",
            "Época 461: perda 0.567306\n",
            "Época 462: perda 0.592156\n",
            "Época 463: perda 0.564228\n",
            "Época 464: perda 0.575357\n",
            "Época 465: perda 0.589036\n",
            "Época 466: perda 0.585297\n",
            "Época 467: perda 0.575438\n",
            "Época 468: perda 0.577706\n",
            "Época 469: perda 0.577907\n",
            "Época 470: perda 0.581001\n",
            "Época 471: perda 0.580561\n",
            "Época 472: perda 0.597333\n",
            "Época 473: perda 0.581218\n",
            "Época 474: perda 0.582427\n",
            "Época 475: perda 0.578635\n",
            "Época 476: perda 0.595819\n",
            "Época 477: perda 0.585649\n",
            "Época 478: perda 0.598519\n",
            "Época 479: perda 0.590821\n",
            "Época 480: perda 0.590448\n",
            "Época 481: perda 0.589133\n",
            "Época 482: perda 0.587923\n",
            "Época 483: perda 0.596762\n",
            "Época 484: perda 0.573376\n",
            "Época 485: perda 0.582034\n",
            "Época 486: perda 0.570753\n",
            "Época 487: perda 0.580423\n",
            "Época 488: perda 0.585740\n",
            "Época 489: perda 0.585973\n",
            "Época 490: perda 0.573759\n",
            "Época 491: perda 0.562780\n",
            "Época 492: perda 0.589556\n",
            "Época 493: perda 0.587612\n",
            "Época 494: perda 0.583312\n",
            "Época 495: perda 0.577875\n",
            "Época 496: perda 0.576403\n",
            "Época 497: perda 0.584448\n",
            "Época 498: perda 0.610953\n",
            "Época 499: perda 0.582274\n",
            "Época 500: perda 0.573256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificando somente um registro"
      ],
      "metadata": {
        "id": "EH75NL2kT5g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_registro = torch.rand((1,4)) # criando um unico registro com 4 features aleatorias"
      ],
      "metadata": {
        "id": "xwHXRHDiT8Ov"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificador.eval() # colocando o modelo em modo avaliacao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbMAPTsqUNF_",
        "outputId": "74357ea2-7c7f-4c58-82fe-9faa087fdafb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificadorTorch(\n",
              "  (dense0): Linear(in_features=4, out_features=16, bias=True)\n",
              "  (dense1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (dense2): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (dense3): Linear(in_features=16, out_features=3, bias=True)\n",
              "  (activation): ReLU()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (output): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # evita fazer calculos desnecessarios de gradiente\n",
        "  saida = classificador(novo_registro)"
      ],
      "metadata": {
        "id": "dCYbB_fGURTE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classe_predita = torch.argmax(saida, dim=1).item() # obtendo a classe com maior probabilidade"
      ],
      "metadata": {
        "id": "mCkwg8pCUVKx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exibindo resultado da predição:\n",
        "print(f'Valores de entrada: {novo_registro}')\n",
        "print(f'valores de saída do modelo: {saida}')\n",
        "print(f'classe prevista pelo modelo: {classe_predita}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4QcPqXkUX6Z",
        "outputId": "fb73543b-805f-4935-b16f-8724888f6165"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores de entrada: tensor([[0.5505, 0.5295, 0.2579, 0.2716]])\n",
            "valores de saída do modelo: tensor([[9.3322e-01, 6.6784e-02, 1.0100e-16]])\n",
            "classe prevista pelo modelo: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando o classificador"
      ],
      "metadata": {
        "id": "CNDrmvh7V-ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classificador.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UHyfum5WAuH",
        "outputId": "9bab2894-5955-42c4-ba1c-4e2b4b30f742"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('dense0.weight',\n",
              "              tensor([[-2.9181e-02, -1.3845e-01,  2.5689e-01,  4.1150e-01],\n",
              "                      [-8.3636e-04, -1.2133e-01,  2.4632e-01,  3.4094e-01],\n",
              "                      [ 1.8702e-01,  2.4925e-01, -2.7884e-01, -3.8794e-01],\n",
              "                      [ 1.5770e-01,  2.8071e-01, -2.9096e-01, -4.2115e-01],\n",
              "                      [-5.2147e-02, -1.7987e-01,  2.7219e-01,  3.8150e-01],\n",
              "                      [ 1.6671e-01,  1.8002e-01, -2.6353e-01, -4.4664e-01],\n",
              "                      [-6.6180e-02, -2.2173e-01,  3.1148e-01,  5.0832e-01],\n",
              "                      [-3.6549e-02, -2.1154e-01,  3.5635e-01,  4.5821e-01],\n",
              "                      [-8.9790e-03, -2.6542e-03, -5.8782e-03, -4.6132e-04],\n",
              "                      [-5.1015e-21,  4.2077e-40,  1.9431e-27, -4.3233e-39],\n",
              "                      [ 1.0702e-01,  2.2001e-01, -2.0757e-01, -4.2032e-01],\n",
              "                      [-1.5152e-02, -2.2686e-01,  3.0582e-01,  4.5120e-01],\n",
              "                      [ 1.3989e-01,  2.2850e-01, -2.5489e-01, -3.8466e-01],\n",
              "                      [-1.7895e-02, -1.1161e-02, -1.6786e-02, -9.0701e-03],\n",
              "                      [ 1.6886e-01,  2.4252e-01, -2.5600e-01, -4.3894e-01],\n",
              "                      [ 1.3265e-01,  2.0079e-01, -2.5479e-01, -3.9542e-01]])),\n",
              "             ('dense0.bias',\n",
              "              tensor([-2.6032e-01, -1.3623e-01,  2.6028e-01,  3.0106e-01, -6.6086e-01,\n",
              "                       5.2313e-01,  1.2243e-01, -5.7336e-03, -9.1190e-05, -8.3453e-40,\n",
              "                       4.7109e-01, -2.1584e-01,  4.7717e-01, -4.2577e-03,  3.2976e-01,\n",
              "                       5.9672e-01])),\n",
              "             ('dense1.weight',\n",
              "              tensor([[ 1.0189e-01,  9.5198e-02, -5.5278e-01, -4.8542e-01,  2.5246e-01,\n",
              "                       -5.3455e-01,  1.5166e-01,  7.8052e-02, -1.0792e-06,  4.8415e-39,\n",
              "                       -4.0774e-01,  1.5816e-01, -5.4755e-01,  5.7648e-04, -5.5556e-01,\n",
              "                       -4.9302e-01],\n",
              "                      [-1.7834e-02,  4.2299e-05, -1.2641e-02, -1.0142e-02,  8.8309e-06,\n",
              "                       -1.2324e-02, -6.5833e-07, -2.4856e-02, -6.4316e-39, -4.1008e-39,\n",
              "                       -1.2996e-02, -2.1359e-02, -1.3596e-02, -4.9105e-40, -1.3304e-02,\n",
              "                       -1.1139e-02],\n",
              "                      [ 1.9691e-01,  1.4120e-01, -3.2716e-02, -3.1939e-02,  1.7183e-01,\n",
              "                       -1.4073e-02,  2.3193e-01,  1.7656e-01, -5.5294e-07, -3.0857e-39,\n",
              "                       -3.7447e-02,  2.2276e-01, -5.7481e-02,  6.4558e-06, -1.4766e-02,\n",
              "                       -2.8455e-02],\n",
              "                      [-6.8155e-02, -4.0503e-02,  8.9165e-01,  8.2434e-01, -9.4204e-02,\n",
              "                        9.3720e-01, -4.3964e-02, -5.1802e-02, -6.2311e-05,  2.9967e-40,\n",
              "                        7.7509e-01, -4.6774e-02,  8.2661e-01, -1.9372e-02,  8.0384e-01,\n",
              "                        9.1161e-01],\n",
              "                      [-1.6750e-02, -2.2871e-02,  8.2077e-01,  7.9433e-01, -1.2287e-02,\n",
              "                        1.0196e+00, -5.4088e-03, -2.2201e-02,  4.3834e-03,  4.9120e-39,\n",
              "                        8.3093e-01,  1.9758e-02,  8.9791e-01,  7.0996e-03,  8.3407e-01,\n",
              "                        9.4004e-01],\n",
              "                      [ 3.7109e-01,  1.9906e-01, -7.8296e-02, -6.2715e-02,  3.3905e-01,\n",
              "                       -8.0979e-02,  3.3441e-01,  3.4885e-01, -7.3232e-10,  4.6789e-39,\n",
              "                       -1.2342e-01,  3.1350e-01, -4.4361e-02,  5.6383e-09, -4.2370e-02,\n",
              "                       -5.7609e-02],\n",
              "                      [ 4.1051e-01,  2.1443e-01, -8.1971e-02, -7.5912e-02,  4.1045e-01,\n",
              "                       -9.2902e-02,  2.9598e-01,  2.9724e-01, -9.3410e-15, -2.3707e-39,\n",
              "                       -1.6559e-01,  3.3364e-01, -1.3491e-01,  6.6832e-06, -6.1188e-02,\n",
              "                       -1.1186e-01],\n",
              "                      [-1.1413e-02, -2.7456e-02,  7.9044e-01,  8.0703e-01,  1.1345e-03,\n",
              "                        9.1705e-01,  5.5653e-04, -1.9514e-02, -5.5975e-04,  1.4996e-39,\n",
              "                        8.0978e-01, -3.4952e-03,  8.8948e-01,  4.7086e-03,  8.2897e-01,\n",
              "                        9.0532e-01],\n",
              "                      [ 2.4008e-01,  2.1467e-01, -1.0620e-01, -4.1379e-02,  3.1567e-01,\n",
              "                       -7.5600e-02,  2.1309e-01,  2.4972e-01, -1.4846e-09, -7.1444e-40,\n",
              "                       -1.1549e-01,  2.8777e-01, -1.1407e-01,  8.8823e-06, -9.0797e-02,\n",
              "                       -5.4805e-02],\n",
              "                      [ 2.3815e-01,  1.8296e-01, -8.9249e-02, -9.4486e-02,  3.2267e-01,\n",
              "                       -9.7264e-02,  3.2193e-01,  2.9574e-01, -2.9912e-11,  2.3937e-39,\n",
              "                       -1.5563e-01,  3.0913e-01, -1.0999e-01,  6.5358e-05, -1.1363e-01,\n",
              "                       -8.5284e-02],\n",
              "                      [ 3.8047e-01,  2.4355e-01, -1.0896e-01, -4.6591e-02,  3.3662e-01,\n",
              "                       -5.5273e-02,  3.1773e-01,  3.2134e-01, -4.8479e-14,  1.1333e-38,\n",
              "                       -9.9312e-02,  3.4665e-01, -1.5241e-01,  5.5904e-07, -6.2050e-02,\n",
              "                       -5.1841e-02],\n",
              "                      [-6.5032e-02, -3.8874e-02,  9.2609e-01,  8.5836e-01, -9.0443e-02,\n",
              "                        9.6859e-01, -8.8653e-03, -6.0634e-02,  7.8242e-03,  6.5273e-39,\n",
              "                        8.6808e-01, -2.7270e-02,  9.2751e-01, -1.6185e-02,  8.8919e-01,\n",
              "                        9.1682e-01],\n",
              "                      [-3.9014e-02, -1.3538e-02,  7.9024e-01,  7.5093e-01, -3.1627e-02,\n",
              "                        9.2162e-01,  2.3819e-02, -3.3554e-02,  6.1585e-03,  3.7949e-39,\n",
              "                        7.7311e-01,  8.6285e-03,  8.6164e-01, -9.2345e-03,  8.0629e-01,\n",
              "                        9.2369e-01],\n",
              "                      [ 3.6176e-01,  2.2090e-01, -1.3716e-01, -8.8546e-02,  4.2883e-01,\n",
              "                       -8.6670e-02,  3.6539e-01,  3.3441e-01, -6.1744e-13, -3.4560e-40,\n",
              "                       -1.1811e-01,  3.8146e-01, -1.5377e-01,  5.0482e-06, -7.8388e-02,\n",
              "                       -6.8655e-02],\n",
              "                      [ 3.7460e-02,  1.1795e-01, -5.5724e-01, -5.4578e-01,  1.3316e-01,\n",
              "                       -7.1870e-01,  2.2733e-02,  4.1852e-02, -3.9627e-04, -5.5521e-39,\n",
              "                       -5.3102e-01,  1.3899e-02, -6.2208e-01,  7.1932e-03, -5.9198e-01,\n",
              "                       -6.6731e-01],\n",
              "                      [ 6.7544e-03,  2.3900e-02,  3.2885e-01,  3.5314e-01,  3.7466e-02,\n",
              "                        3.5610e-01, -6.3670e-03, -9.2333e-03,  5.4713e-14,  4.8651e-39,\n",
              "                        3.2821e-01,  7.2375e-03,  3.7685e-01, -9.7807e-05,  3.4215e-01,\n",
              "                        3.6827e-01]])),\n",
              "             ('dense1.bias',\n",
              "              tensor([ 0.2363, -0.0198,  0.1598,  0.2612, -0.0131,  0.2436,  0.4447,  0.0286,\n",
              "                       0.4134,  0.4125,  0.4333,  0.1544, -0.0051,  0.3666,  0.1386, -0.0116])),\n",
              "             ('dense2.weight',\n",
              "              tensor([[-9.7030e-02,  1.3112e-05, -5.9481e-02,  7.5328e-01,  6.9317e-01,\n",
              "                       -6.5443e-03, -3.3129e-02,  7.4387e-01, -2.3831e-02, -2.2532e-02,\n",
              "                       -2.8102e-02,  7.4869e-01,  6.3098e-01, -2.4523e-03, -3.1113e-01,\n",
              "                        1.5824e-01],\n",
              "                      [-5.8501e-03, -5.6292e-04, -2.6765e-01,  8.9427e-02,  6.7201e-02,\n",
              "                       -3.1876e-01, -2.8831e-01,  6.2899e-02, -3.2542e-01, -2.9419e-01,\n",
              "                       -3.2512e-01,  7.2692e-02,  4.0307e-02, -2.9654e-01, -5.9042e-07,\n",
              "                        2.2084e-01],\n",
              "                      [ 2.8321e-02,  2.7081e-03,  3.8359e-01, -4.6371e-03, -8.9412e-03,\n",
              "                        3.8764e-01,  3.8419e-01,  1.3806e-03,  2.0771e-01,  2.3521e-01,\n",
              "                        4.0566e-01, -7.9890e-03, -7.5678e-03,  3.5584e-01, -3.9511e-02,\n",
              "                       -4.2997e-02],\n",
              "                      [ 2.1343e-01,  2.0240e-07,  1.2421e-01, -4.6474e-01, -5.3055e-01,\n",
              "                        2.0979e-02,  1.0420e-01, -5.8924e-01,  5.7406e-02,  9.6699e-02,\n",
              "                        7.5853e-02, -5.0467e-01, -5.1693e-01,  3.5739e-02,  4.9468e-01,\n",
              "                       -2.7299e-01],\n",
              "                      [ 1.4553e-01, -1.0285e-06,  1.2043e-02, -5.6530e-01, -6.7510e-01,\n",
              "                        2.0256e-02,  4.5881e-02, -7.1154e-01,  6.0592e-02,  6.8870e-02,\n",
              "                        8.6864e-02, -6.7055e-01, -6.4442e-01,  9.3085e-02,  4.1473e-01,\n",
              "                       -1.9472e-01],\n",
              "                      [-1.1686e-01, -2.7701e-03, -3.0102e-01,  9.2207e-02,  3.7591e-02,\n",
              "                       -4.6631e-01, -4.5784e-01,  8.4640e-02, -4.5753e-01, -4.1468e-01,\n",
              "                       -4.7694e-01,  7.0461e-02,  8.7339e-02, -4.4392e-01, -1.0321e-04,\n",
              "                        8.7078e-02],\n",
              "                      [ 2.1732e-01,  3.3564e-08,  3.5176e-02, -6.6473e-01, -6.8515e-01,\n",
              "                        3.6969e-02,  2.9057e-02, -6.8030e-01,  8.6512e-02,  1.2860e-01,\n",
              "                        3.7629e-02, -6.8357e-01, -6.2204e-01,  7.4821e-02,  3.4655e-01,\n",
              "                       -2.0535e-01],\n",
              "                      [-8.0096e-03,  8.3001e-05, -3.0895e-01,  6.0764e-02,  9.8233e-02,\n",
              "                       -3.7041e-01, -3.1868e-01,  1.0257e-01, -3.3474e-01, -3.2935e-01,\n",
              "                       -3.8659e-01,  9.1699e-02,  1.2616e-01, -3.7135e-01, -8.6537e-07,\n",
              "                        1.6980e-01],\n",
              "                      [-9.5516e-02,  3.1861e-08, -3.7582e-02,  6.6738e-01,  6.8042e-01,\n",
              "                        3.9673e-03, -3.4517e-02,  7.4075e-01,  3.5334e-03, -1.4441e-02,\n",
              "                       -3.0751e-02,  6.5263e-01,  6.0192e-01,  1.9247e-03, -2.8302e-01,\n",
              "                        1.5398e-01],\n",
              "                      [ 2.3598e-01,  1.1865e-07,  8.7280e-02, -2.6521e-01, -2.1901e-01,\n",
              "                        8.6721e-02,  1.3014e-01, -2.3817e-01,  7.1469e-02,  1.4490e-01,\n",
              "                        9.4388e-02, -2.1083e-01, -2.2158e-01,  2.3937e-02,  3.1778e-01,\n",
              "                       -1.9873e-01],\n",
              "                      [-1.3783e-01, -1.4215e-05,  3.5313e-01, -1.2732e-02, -1.6432e-02,\n",
              "                        3.5529e-01,  2.0671e-01,  1.2747e-02,  2.3726e-01,  2.0141e-01,\n",
              "                        3.0042e-01,  1.0238e-03, -2.2817e-03,  2.9253e-01, -4.0090e-01,\n",
              "                       -6.0094e-02],\n",
              "                      [-1.2900e-01,  1.4501e-05,  1.6008e-02,  6.2727e-01,  6.3577e-01,\n",
              "                        1.1907e-03, -1.9985e-04,  6.6969e-01,  2.0948e-02, -6.7341e-03,\n",
              "                        4.8815e-03,  6.2316e-01,  6.4352e-01, -1.6780e-02, -3.2089e-01,\n",
              "                        4.3254e-01],\n",
              "                      [ 1.7524e-01,  7.0502e-08,  9.4295e-02, -6.1544e-01, -6.7387e-01,\n",
              "                        5.2922e-02,  5.0354e-02, -6.6474e-01,  6.2054e-02,  9.0545e-02,\n",
              "                        4.0171e-02, -6.4342e-01, -6.5161e-01,  3.4961e-02,  6.2470e-01,\n",
              "                       -1.9989e-01],\n",
              "                      [ 2.9892e-01,  2.3835e-03,  3.1728e-01, -1.2307e-02, -2.9171e-02,\n",
              "                        5.0444e-01,  4.7763e-01,  4.5632e-04,  4.0334e-01,  4.3537e-01,\n",
              "                        4.8088e-01, -1.8243e-02, -2.3514e-02,  4.5759e-01,  1.6400e-01,\n",
              "                       -4.4945e-02],\n",
              "                      [ 1.6215e-01,  4.1116e-03,  2.9108e-01, -1.6267e-02, -1.4641e-02,\n",
              "                        5.3401e-01,  4.4242e-01,  3.8058e-03,  4.6228e-01,  4.4692e-01,\n",
              "                        5.4219e-01, -2.0373e-02, -1.6571e-02,  4.7179e-01, -5.9399e-02,\n",
              "                       -4.9446e-02],\n",
              "                      [ 1.8560e-01,  3.7753e-03,  3.6350e-01, -1.2957e-03, -3.0228e-04,\n",
              "                        3.8794e-01,  4.1567e-01, -4.7461e-03,  4.0950e-01,  4.0755e-01,\n",
              "                        4.6891e-01, -1.1494e-02, -1.5065e-02,  4.1898e-01, -4.4523e-02,\n",
              "                       -3.0139e-02]])),\n",
              "             ('dense2.bias',\n",
              "              tensor([0.3715, 0.0570, 0.0785, 0.2097, 0.1139, 0.2276, 0.0266, 0.0248, 0.1955,\n",
              "                      0.1043, 0.1720, 0.0086, 0.2193, 0.1060, 0.0419, 0.0558])),\n",
              "             ('dense3.weight',\n",
              "              tensor([[ 0.3164,  0.4073, -1.0190, -0.0299, -0.0186,  0.6108, -0.1472,  0.3934,\n",
              "                        0.3837, -0.0140, -0.8566,  0.2716, -0.0341, -0.9892, -0.9939, -1.0295],\n",
              "                      [ 0.2536, -0.3324,  0.0090, -0.3930, -0.7208, -0.4493, -0.5592, -0.3297,\n",
              "                        0.3150, -0.2000,  0.0765,  0.2109, -0.5664, -0.0197,  0.1768,  0.1029],\n",
              "                      [-0.9343, -0.1378,  0.0234,  0.3860,  0.7286, -0.2335,  0.6078, -0.2186,\n",
              "                       -0.9898,  0.2011, -0.0062, -0.7405,  0.5721,  0.0528,  0.1652,  0.0796]])),\n",
              "             ('dense3.bias', tensor([ 0.1698, -0.1142, -0.3423]))])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# salvando o classificador com o nome 'classificador_iris' na extensao .pth\n",
        "torch.save(classificador.state_dict(), 'classificador_iris.pth')"
      ],
      "metadata": {
        "id": "JphWjTxaWEID"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}